{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Understanding CNNs\n",
    "Study the provided resources to understand the basic concepts of CNNs, convolutional layers, and pooling layers, and answer the following questions.\n",
    "   - What are the advantages of convolutional layers over fully connected layers in image processing tasks?\n",
    "   - How does pooling help in reducing the computational complexity of a CNN?\n",
    "   - Compare different types of pooling layers (max pooling, average pooling). What are their respective advantages and disadvantages?\n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What are the advantages of convolutional layers over fully connected layers in image processing tasks?\n",
    "\n",
    " Advantages of Convolutional Layers Over Fully Connected Layers in Image Processing Tasks\n",
    "\n",
    " 1. Convolutional Layers:\n",
    "\n",
    "- Local Receptive Fields: Convolutional layers use local receptive fields, which means they apply filters to local regions of the input image. This allows the network to capture local patterns (such as edges, textures) that are crucial for image recognition.\n",
    "\n",
    "- Parameter Sharing: Instead of learning a separate set of weights for each pixel, convolutional layers use the same filter (kernel) across the entire image. This reduces the number of parameters and makes the model more efficient.\n",
    "\n",
    "- Translation Invariance: Convolutional layers help in detecting features regardless of their position in the image. This means the network can recognize objects even if they are shifted within the image.\n",
    "\n",
    "- Reduced Computational Cost: Because convolutional layers use shared weights and local connectivity, they significantly reduce the number of computations compared to fully connected layers, which connect every neuron to every neuron in the next layer.\n",
    "\n",
    "2. Fully Connected Layers:\n",
    "\n",
    "- High Parameter Count: In fully connected layers, every neuron is connected to every neuron in the subsequent layer. This results in a large number of parameters, making the network prone to overfitting and increasing computational cost.\n",
    "\n",
    "- No Spatial Hierarchy: Fully connected layers do not take into account the spatial hierarchy of the input data. They treat each pixel independently, which can be less effective for capturing patterns in image data.\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "How does pooling help in reducing the computational complexity of a CNN?\n",
    "\n",
    "1. Pooling:\n",
    "\n",
    "- Dimensionality Reduction: Pooling layers reduce the spatial dimensions (width and height) of the feature maps. This leads to a reduction in the number of parameters and computations needed for the following layers.\n",
    "\n",
    "- Feature Extraction: Pooling helps in retaining the most important features while discarding less important ones. \n",
    "\n",
    "- It consolidates information and retains the essential characteristics of the features detected by the convolutional layers.\n",
    "\n",
    "- Reduced Overfitting: By reducing the spatial dimensions and the number of parameters, pooling layers can help prevent overfitting, as the model becomes less likely to memorize the training data and more likely to generalize well.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "Compare different types of pooling layers (max pooling, average pooling). What are their respective advantages and disadvantages?\n",
    "\n",
    "\n",
    "\n",
    "Comparison of Different Types of Pooling Layers\n",
    "\n",
    "1. Max Pooling:\n",
    "\n",
    " Max pooling involves selecting the maximum value from a group of values in the feature map.\n",
    "\n",
    "- Advantages:\n",
    "Feature Preservation: It retains the most prominent feature in each region, which can be crucial for detecting important patterns.\n",
    "\n",
    "Translation Invariance: Helps in achieving translation invariance by preserving the most significant features.\n",
    "- Disadvantages:\n",
    "Loss of Information: Since only the maximum value is retained, other valuable information in the pooled region is discarded.\n",
    "\n",
    "2. Average Pooling:\n",
    "\n",
    " Average pooling involves taking the average value from a group of values in the feature map.\n",
    "\n",
    "- Advantages:\n",
    "Smoothing Effect: It can produce smoother and more generalized feature maps, which may be beneficial in some contexts.\n",
    "Less Prone to Overfitting: By averaging values, average pooling might reduce the risk of overfitting compared to max pooling.\n",
    "\n",
    "- Disadvantages:\n",
    "Less Discriminative: It might not preserve the most salient features as effectively as max pooling, potentially leading to a loss of important details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 38ms/step - accuracy: 0.8510 - loss: 0.4778 - val_accuracy: 0.9782 - val_loss: 0.0761\n",
      "Epoch 2/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 42ms/step - accuracy: 0.9816 - loss: 0.0596 - val_accuracy: 0.9822 - val_loss: 0.0588\n",
      "Epoch 3/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 42ms/step - accuracy: 0.9874 - loss: 0.0403 - val_accuracy: 0.9877 - val_loss: 0.0430\n",
      "Epoch 4/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.9910 - loss: 0.0288 - val_accuracy: 0.9894 - val_loss: 0.0386\n",
      "Epoch 5/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.9931 - loss: 0.0220 - val_accuracy: 0.9902 - val_loss: 0.0346\n",
      "Epoch 6/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 30ms/step - accuracy: 0.9931 - loss: 0.0202 - val_accuracy: 0.9890 - val_loss: 0.0368\n",
      "Epoch 7/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 31ms/step - accuracy: 0.9958 - loss: 0.0137 - val_accuracy: 0.9889 - val_loss: 0.0467\n",
      "Epoch 8/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9960 - loss: 0.0124 - val_accuracy: 0.9884 - val_loss: 0.0451\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0523\n",
      "Test Accuracy: 0.9879000186920166\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Implementing a CNN on MNIST\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=64,\n",
    "                    validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "# Evaluation Code:\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture Details:\n",
    "\n",
    "Conv2D Layer 1: 32 filters, 3x3 kernel size, ReLU activation, input shape (28, 28, 1)\n",
    "MaxPooling2D Layer 1: 2x2 pool size\n",
    "Conv2D Layer 2: 64 filters, 3x3 kernel size, ReLU activation\n",
    "MaxPooling2D Layer 2: 2x2 pool size\n",
    "Conv2D Layer 3: 64 filters, 3x3 kernel size, ReLU activation\n",
    "Flatten Layer: Flattens the 3D tensor to 1D\n",
    "Dense Layer 1: 64 units, ReLU activation\n",
    "Dense Layer 2 (Output): 10 units, softmax activation (for 10 classes)\n",
    "\n",
    "\n",
    "Training Details:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Reshape the images to (28, 28, 1) for grayscale.\n",
    "Normalize pixel values to the range [0, 1].\n",
    "Convert labels to categorical format.\n",
    "Training Parameters:\n",
    "\n",
    "Optimizer: Adam\n",
    "Loss Function: Categorical Crossentropy\n",
    "Metrics: Accuracy\n",
    "Batch Size: 64\n",
    "Epochs: 10\n",
    "Validation Split: 20% of training data\n",
    "Callbacks:\n",
    "\n",
    "EarlyStopping: Monitors validation loss with a patience of 3 epochs.\n",
    "ModelCheckpoint: Saves the best model based on validation loss in the .keras format.\n",
    "Challenges Faced:\n",
    "\n",
    "Overfitting: Adjusted the number of epochs and added early stopping to prevent overfitting.\n",
    "Hyperparameter Tuning: Experimented with different batch sizes and learning rates for better performance.\n",
    "\n",
    "\n",
    "Evaluation Details:\n",
    "\n",
    "Test Accuracy and Loss: Outputs the model's performance on the test set.\n",
    "Plots: Visualizes training and validation accuracy and loss to assess learning curves and detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,940,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m18,940,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 25000 images belonging to 2 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m333/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10:57\u001b[0m 1s/step - accuracy: 0.5293 - loss: 0.7414"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x0000016C4864AA20>\nTraceback (most recent call last):\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n          ~~~~~~~~~~~~~~~^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py\", line 3498, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x0000016C4864AA20>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_55002]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 73\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     69\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     70\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x0000016C4864AA20>\nTraceback (most recent call last):\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 260, in _get_iterator\n    for i, batch in enumerate(gen_fn()):\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py\", line 253, in generator_fn\n    yield self.py_dataset[i]\n          ~~~~~~~~~~~~~~~^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 68, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py\", line 3498, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x0000016C4864AA20>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_55002]"
     ]
    }
   ],
   "source": [
    "#Implementing a CNN on CAT vs DOG\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Convolutional Layer 3\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten Layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer 1\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\DELL\\Desktop\\BWT-ML-DL\\WEEK-9\\dataset\\train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    r'C:\\Users\\DELL\\Desktop\\BWT-ML-DL\\WEEK-9\\dataset\\validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_acc = model.evaluate(validation_generator, verbose=2)\n",
    "print(f'Validation accuracy: {val_acc:.4f}')\n",
    "print(f'Validation loss: {val_loss:.4f}')\n",
    "\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, max(history.history['loss'])])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture Details:\n",
    "\n",
    "Conv2D Layer 1: 32 filters, 3x3 kernel size, ReLU activation, input shape (150, 150, 3) (RGB images)\n",
    "MaxPooling2D Layer 1: 2x2 pool size\n",
    "Conv2D Layer 2: 64 filters, 3x3 kernel size, ReLU activation\n",
    "MaxPooling2D Layer 2: 2x2 pool size\n",
    "Conv2D Layer 3: 128 filters, 3x3 kernel size, ReLU activation\n",
    "MaxPooling2D Layer 3: 2x2 pool size\n",
    "Flatten Layer: Flattens the 3D tensor to 1D\n",
    "Dense Layer 1: 512 units, ReLU activation\n",
    "Dense Layer 2 (Output): 1 unit, sigmoid activation (for binary classification)\n",
    "\n",
    "\n",
    "\n",
    "Training Details:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "Training Data Augmentation: Rescaling, shearing, zooming, and horizontal flipping.\n",
    "Test Data Preprocessing: Rescaling only.\n",
    "Training Parameters:\n",
    "\n",
    "Optimizer: Adam\n",
    "Loss Function: Binary Crossentropy\n",
    "Metrics: Accuracy\n",
    "Batch Size: 32\n",
    "Epochs: 20\n",
    "Callbacks:\n",
    "EarlyStopping: Monitors validation loss with a patience of 5 epochs.\n",
    "ModelCheckpoint: Saves the best model based on validation loss in the .keras format.\n",
    "Challenges Faced:\n",
    "\n",
    "Overfitting: Adjusted data augmentation techniques and used early stopping to mitigate overfitting.\n",
    "Computational Resources: Training on a large dataset can be resource-intensive; consider using GPU acceleration if available.\n",
    "\n",
    "\n",
    "\n",
    "Evaluation Details:\n",
    "\n",
    "Validation Accuracy and Loss: Outputs the model's performance on the validation set.\n",
    "Plots: Visualizes training and validation accuracy and loss to assess learning curves and detect overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
